---
title: "Tree_Diagrams"
author: "Laura Larregui, John Christman, Angela Rivera"
date: "12/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Taking a look at the correlation coefficients from the previous file, we see that density is moderately correlated with fixed.acidity (r=0.67) and alcohol (r=âˆ’0.5). Fixed.acidity is moderately correlated with citric.acid (r= 0.67) and ph (r=-0.68). Citric.acid is moderately correlated with volatile.acidity (r=-0.55) and ph (r=-0.54). Free.sulfur.dioxide and total.sulfur.dioxide are also moderately correlated with each other (r=0.67) although this is trivially known because free sulfur dioxide is incorporated into the total sulfur dioxide. Aside from that correlations are all very low, including quality.

```{r visualization}
readURL <- function(inputURL)  #Begin function named readURL that takes a URL
{
  csvFile <- read.csv(url(inputURL), sep = ';')  #assign the results of the URL call as a csv file to a dataframe named csvFile. Added sep = ';' to seperate the data into columns
  return(csvFile)  # return the dataframe
}
redWine <- readURL("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv")
whiteWine <- readURL("https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv")
# Verify no NAs
redWine <- na.omit(redWine)
whiteWine<-na.omit (whiteWine)
#table preview
knitr::kable(head(redWine))
#install.packages("plotly")
library(plotly) #data visualization
redWine$bquality <- ifelse(redWine$quality < 5, "Mediocre", ifelse(redWine$quality <7 , "Average", ifelse(redWine$quality >6, "Excellent", NA)))
plot_ly(data = redWine, x = ~bquality, y = ~alcohol, color = ~bquality, type = "box", colors = "Dark2")
```
```{r trees}
#install.packages("rpart")
#install.packages("rpart.plot")
#install.packages("rattle")
library(rpart)
library(rpart.plot)
library(rattle)
#Recursive Partitioning and Regression Trees
#Reference:https://www.rdocumentation.org/packages/rpart/versions/4.1-15/topics/rpart
nrows <- nrow(redWine)
cutPoint <- floor(nrows/3*2)
rand <- sample(1:nrows)
#training set
red_train <- redWine[rand[1:cutPoint], ]
#test set
red_test <- redWine[rand[(cutPoint+1:nrows)], ]
w.rpart <- rpart(quality ~. , data = red_train)
w.rpart
rpart.plot(w.rpart, digits = 3)
fancyRpartPlot(w.rpart)
prediction <- predict(w.rpart,red_test)
summary(prediction)
summary(red_test$quality)
        
```
```{r redForest}
library(randomForest)
set.seed(100)
red_rftrain <- na.omit(sample(nrow(redWine), 0.7*nrow(redWine), replace=FALSE))
TrainSet <- na.omit(redWine[red_rftrain,])
TestSet <- na.omit(redWine[-red_rftrain,])
TrainSet <- TrainSet[,-13]
TestSet <- TestSet[,-13]

str(TrainSet)
str(TestSet)
rw1 <- randomForest(quality ~ .,data = TrainSet, importance = TRUE)

rw1
rw2 <-  randomForest(quality ~ .,data = TrainSet, ntree = 500, mtry = 6, importance = TRUE)

rw2
importance(rw1)
importance(rw2)

predTrainSet <- predict(rw2, TrainSet, type = "class")
table(predTrainSet, TrainSet$quality)

predTestSet <- predict(rw2, TestSet, type = "class")
mean(predTestSet == TestSet$quality)
table(predTestSet, TestSet$quality)
# TO DO
#make better models.  Only predicting about 47%
#Fix importance tables.  Now showing data out to 15 decimal places.  
#run on White wine data set.
```

